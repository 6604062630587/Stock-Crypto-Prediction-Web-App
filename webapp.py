import streamlit as st
from streamlit_option_menu import option_menu
import joblib
import pandas as pd
import numpy as np
import yfinance as yf
import tensorflow as tf
import requests
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, MinMaxScaler
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor
from xgboost import XGBRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
import os
import matplotlib.dates as mdates
from tensorflow.keras.models import Sequential, load_model
from tensorflow.keras.layers import LSTM, Dense, Dropout, Conv1D, MaxPooling1D, Flatten, BatchNormalization
from tensorflow.keras.callbacks import EarlyStopping


def fetch_binance_data(symbol="BNBUSDT", interval="1d", limit=1000):
    url = f"https://api.binance.com/api/v3/klines?symbol={symbol}&interval={interval}&limit={limit}"
    response = requests.get(url)
    data = response.json()

    df = pd.DataFrame(data, columns=["timestamp", "open", "high", "low", "close", "volume", "close_time",
                      "quote_asset_volume", "trades", "taker_buy_base_asset_volume", "taker_buy_quote_asset_volume", "ignore"])
    df = df[["timestamp", "open", "high", "low", "close", "volume", "trades"]]
    df["timestamp"] = pd.to_datetime(df["timestamp"], unit="ms")
    df[["open", "high", "low", "close", "volume", "trades"]] = df[[
        "open", "high", "low", "close", "volume", "trades"]].astype(float)
    return df


def add_indicators(df):
    df["MA_7"] = df["close"].rolling(window=7).mean()
    df["MA_14"] = df["close"].rolling(window=14).mean()
    df["EMA_7"] = df["close"].ewm(span=7, adjust=False).mean()
    df["EMA_14"] = df["close"].ewm(span=14, adjust=False).mean()
    df["RSI_14"] = 100 - (100 / (1 + (df["close"].diff(1).clip(lower=0).rolling(
        14).mean() / df["close"].diff(1).clip(upper=0).abs().rolling(14).mean())))
    df["Daily_Return"] = df["close"].pct_change() * 100
    df.dropna(inplace=True)
    return df


def train_and_save_model(df, symbol):
    features = ["MA_7", "MA_14", "EMA_7", "EMA_14", "RSI_14", "Daily_Return"]
    target = "close"
    X = df[features]
    y = df[target]

    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42)
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)

    models = {
        "Linear Regression": LinearRegression(),
        "Random Forest": RandomForestRegressor(n_estimators=100, random_state=42),
        "XGBoost": XGBRegressor(n_estimators=100, learning_rate=0.1, random_state=42)
    }

    results = {}  # ‡πÄ‡∏Å‡πá‡∏ö‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏Ç‡∏≠‡∏á‡πÅ‡∏ï‡πà‡∏•‡∏∞‡πÇ‡∏°‡πÄ‡∏î‡∏•
    for name, model in models.items():
        model.fit(X_train_scaled, y_train)
        y_pred = model.predict(X_test_scaled)
        mae = mean_absolute_error(y_test, y_pred)
        r2 = r2_score(y_test, y_pred)
        results[name] = {"MAE": mae, "R¬≤ Score": r2}

    best_model_name = max(results, key=lambda x: results[x]["R¬≤ Score"])
    best_model = models[best_model_name]
    model_filename = f"{symbol.lower()}_best_model.pkl"

    joblib.dump(best_model, model_filename)  # ‚úÖ ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÇ‡∏°‡πÄ‡∏î‡∏•
    joblib.dump(scaler, f"{symbol.lower()}_scaler.pkl")  # ‚úÖ ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å scaler

    return model_filename, results


st.set_page_config(page_title="Stock & Crypto Prediction Web App", page_icon="üìà", layout="wide")


with st.sidebar:
    selected = option_menu("Main Menu", ["üè† Home", "üìä Cryptocurrency Price Prediction(ML model)", "üìàUS Stock Price Prediction (NN model)", "üìñ Machine Learning Model Development", "üìñ Neural Network Model Development"],
                           icons=["house", "graph-up", "graph-up", "book","book"], menu_icon="menu-hamburger", default_index=0)


if selected == "üè† Home":
    st.title("üìà Stock & Crypto Prediction Web App")
    st.write("""
    ‡∏¢‡∏¥‡∏ô‡∏î‡∏µ‡∏ï‡πâ‡∏≠‡∏ô‡∏£‡∏±‡∏ö‡∏™‡∏π‡πà **Stock & Cryptocurrency Prediction Web App**  
    ‡πÄ‡∏ß‡πá‡∏ö‡∏ô‡∏µ‡πâ‡∏ä‡πà‡∏ß‡∏¢‡πÉ‡∏´‡πâ‡∏Ñ‡∏∏‡∏ì‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ **‡∏û‡∏¢‡∏≤‡∏Å‡∏£‡∏ì‡πå‡∏£‡∏≤‡∏Ñ‡∏≤‡∏´‡∏∏‡πâ‡∏ô‡πÅ‡∏•‡∏∞‡πÄ‡∏´‡∏£‡∏µ‡∏¢‡∏ç‡∏Ñ‡∏£‡∏¥‡∏õ‡πÇ‡∏ï** ‡πÑ‡∏î‡πâ‡πÇ‡∏î‡∏¢‡πÉ‡∏ä‡πâ **Machine Learning ‡πÅ‡∏•‡∏∞ Neural Networks**  
    """)

    # üîπ **1. ‡∏ü‡∏µ‡πÄ‡∏à‡∏≠‡∏£‡πå‡∏´‡∏•‡∏±‡∏Å‡∏Ç‡∏≠‡∏á‡πÄ‡∏ß‡πá‡∏ö**
    st.header("üîπ ‡∏ü‡∏µ‡πÄ‡∏à‡∏≠‡∏£‡πå‡∏´‡∏•‡∏±‡∏Å‡∏Ç‡∏≠‡∏á‡πÄ‡∏ß‡πá‡∏ö")
    st.write("""
    - üìä **‡∏û‡∏¢‡∏≤‡∏Å‡∏£‡∏ì‡πå‡∏£‡∏≤‡∏Ñ‡∏≤‡∏Ñ‡∏£‡∏¥‡∏õ‡πÇ‡∏ï** ‚Üí ‡πÉ‡∏ä‡πâ **Linear Regression, Random Forest ‡πÅ‡∏•‡∏∞ XGBoost**  
    - üìà **‡∏û‡∏¢‡∏≤‡∏Å‡∏£‡∏ì‡πå‡∏£‡∏≤‡∏Ñ‡∏≤‡∏´‡∏∏‡πâ‡∏ô** ‚Üí ‡πÉ‡∏ä‡πâ **LSTM ‡πÅ‡∏•‡∏∞ CNN**  
    - üìñ **‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö‡πÇ‡∏°‡πÄ‡∏î‡∏• Machine Learning & Neural Networks**  
    - üöÄ **‡∏ó‡∏î‡∏•‡∏≠‡∏á‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÅ‡∏ö‡∏ö‡πÄ‡∏£‡∏µ‡∏¢‡∏•‡πÑ‡∏ó‡∏°‡πå**  
    """)

    # üîπ **2. ‡∏Ñ‡∏≥‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô**
    st.header("üîπ ‡∏ß‡∏¥‡∏ò‡∏µ‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô")
    st.write("""
    1Ô∏è‚É£ **‡πÑ‡∏õ‡∏ó‡∏µ‡πà‡∏´‡∏ô‡πâ‡∏≤ "Cryptocurrency Price Prediction"**  
       - ‡∏õ‡πâ‡∏≠‡∏ô‡∏ä‡∏∑‡πà‡∏≠‡πÄ‡∏´‡∏£‡∏µ‡∏¢‡∏ç‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå ‡πÄ‡∏ä‡πà‡∏ô **BNB, DOGE, ETH**  
       - ‡∏£‡∏∞‡∏ö‡∏ö‡∏à‡∏∞‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å Binance API ‡πÅ‡∏•‡∏∞‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏£‡∏≤‡∏Ñ‡∏≤‡∏ï‡∏≤‡∏°‡πÇ‡∏°‡πÄ‡∏î‡∏• Machine Learning  

    2Ô∏è‚É£ **‡πÑ‡∏õ‡∏ó‡∏µ‡πà‡∏´‡∏ô‡πâ‡∏≤ "US Stock Price Prediction"**  
       - ‡∏õ‡πâ‡∏≠‡∏ô‡∏™‡∏±‡∏ç‡∏•‡∏±‡∏Å‡∏©‡∏ì‡πå‡∏´‡∏∏‡πâ‡∏ô ‡πÄ‡∏ä‡πà‡∏ô **AAPL, TSLA, MSFT**  
       - ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÇ‡∏°‡πÄ‡∏î‡∏• **LSTM ‡∏´‡∏£‡∏∑‡∏≠ CNN** ‡πÅ‡∏•‡∏∞‡∏Å‡∏î‡∏õ‡∏∏‡πà‡∏° "üîç ‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢"  

    3Ô∏è‚É£ **‡∏®‡∏∂‡∏Å‡∏©‡∏≤‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Å‡∏≤‡∏£‡∏û‡∏±‡∏í‡∏ô‡∏≤‡πÇ‡∏°‡πÄ‡∏î‡∏•**  
       - ‡πÑ‡∏õ‡∏ó‡∏µ‡πà‡∏´‡∏ô‡πâ‡∏≤ üìñ **Machine Learning Model Development** ‡∏´‡∏£‡∏∑‡∏≠ **Neural Network Model Development**  
       - ‡∏®‡∏∂‡∏Å‡∏©‡∏≤‡∏ß‡∏¥‡∏ò‡∏µ‡∏Å‡∏≤‡∏£‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•, ‡∏Å‡∏≤‡∏£‡∏û‡∏±‡∏í‡∏ô‡∏≤‡πÇ‡∏°‡πÄ‡∏î‡∏• ‡πÅ‡∏•‡∏∞‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏Å‡∏≤‡∏£‡∏ó‡∏î‡∏™‡∏≠‡∏ö  
    """)




elif selected == "üìä Cryptocurrency Price Prediction(ML model)":
    st.title("üìä Cryptocurrency Price Prediction")
    crypto_symbol = st.sidebar.text_input(
        "üî† ‡πÉ‡∏™‡πà‡∏ä‡∏∑‡πà‡∏≠‡∏¢‡πà‡∏≠‡πÄ‡∏´‡∏£‡∏µ‡∏¢‡∏ç (‡πÄ‡∏ä‡πà‡∏ô BNB, DOGE)", value="BNB").upper()

    st.write(f"üîÑ ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• {crypto_symbol} ‡∏à‡∏≤‡∏Å Binance API...")
    df = fetch_binance_data(symbol=f"{crypto_symbol}USDT")
    df = add_indicators(df)

    model_filename = f"{crypto_symbol.lower()}_best_model.pkl"
    if not os.path.exists(model_filename):
        st.write("üöÄ ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡πÄ‡∏ó‡∏£‡∏ô‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÉ‡∏´‡∏°‡πà...")
        model_filename, results = train_and_save_model(df, crypto_symbol)
        model = joblib.load(model_filename)

    model = joblib.load(model_filename)
    st.write("‚úÖ ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÇ‡∏´‡∏•‡∏î‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à!")

    st.sidebar.header("üî¢ ‡πÉ‡∏™‡πà‡∏Ñ‡πà‡∏≤ Indicator")
    default_values = {"MA_7": df["MA_7"].iloc[-1], "MA_14": df["MA_14"].iloc[-1], "EMA_7": df["EMA_7"].iloc[-1],
                      "EMA_14": df["EMA_14"].iloc[-1], "RSI_14": df["RSI_14"].iloc[-1], "Daily_Return": df["Daily_Return"].iloc[-1]}
    features = {key: st.sidebar.number_input(f"{key}", value=float(
        value), step=0.1) for key, value in default_values.items()}

    if st.button(f"üîÆ Predict {crypto_symbol} Price"):
        scaler = joblib.load(f"{crypto_symbol.lower()}_scaler.pkl")
        model = joblib.load(f"{crypto_symbol.lower()}_best_model.pkl")

        X_input_scaled = scaler.transform([list(features.values())])
        predicted_price = model.predict(X_input_scaled)[0]

        st.success(
            f"üí∞ ‡∏£‡∏≤‡∏Ñ‡∏≤‡∏ó‡∏µ‡πà‡∏Ñ‡∏≤‡∏î‡∏Å‡∏≤‡∏£‡∏ì‡πå‡∏Ç‡∏≠‡∏á {crypto_symbol}: ${predicted_price:.2f}")

    st.subheader(f"üìä ‡∏Å‡∏£‡∏≤‡∏ü‡∏£‡∏≤‡∏Ñ‡∏≤‡∏Ç‡∏≠‡∏á {crypto_symbol}")
    fig, ax = plt.subplots()
    ax.plot(df["timestamp"], df["close"], label="Actual Price", color="blue")

    # ‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡∏Ç‡∏≠‡∏á‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà‡πÉ‡∏´‡πâ‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏°
    ax.xaxis.set_major_locator(mdates.AutoDateLocator())
    ax.xaxis.set_major_formatter(mdates.DateFormatter("%Y-%m-%d"))
    plt.xticks(rotation=45)  # ‡∏´‡∏°‡∏∏‡∏ô‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà‡πÉ‡∏´‡πâ‡∏î‡∏π‡∏á‡πà‡∏≤‡∏¢‡∏Ç‡∏∂‡πâ‡∏ô

    ax.set_xlabel("Date")
    ax.set_ylabel("Price (USD)")
    ax.legend()
    st.pyplot(fig)

    # ‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡πÉ‡∏ä‡πâ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡πÄ‡∏ó‡∏£‡∏ô‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÅ‡∏•‡∏∞‡∏£‡∏±‡∏ö‡∏Ñ‡πà‡∏≤‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå
    model_filename, results = train_and_save_model(df, crypto_symbol)

    # ‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏• Model Performance ‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡∏≤‡∏£‡∏≤‡∏á
    st.subheader("üìä Model Performance Comparison")

    # ‡∏™‡∏£‡πâ‡∏≤‡∏á DataFrame ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏Ç‡∏≠‡∏á‡πÇ‡∏°‡πÄ‡∏î‡∏•
    results_df = pd.DataFrame(results).T  # Transpose ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡∏ä‡∏∑‡πà‡∏≠‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÄ‡∏õ‡πá‡∏ô‡πÅ‡∏ñ‡∏ß
    st.dataframe(results_df)

    # ‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏• Model Performance ‡πÄ‡∏õ‡πá‡∏ô‡∏Å‡∏£‡∏≤‡∏ü
    fig, ax = plt.subplots(1, 2, figsize=(12, 5))

    # ‡∏Å‡∏£‡∏≤‡∏ü MAE
    ax[0].bar(results.keys(), [res["MAE"]
                               for res in results.values()], color=['red', 'blue', 'green'])
    ax[0].set_title("Mean Absolute Error (MAE)")
    ax[0].set_ylabel("MAE")

    # ‡∏Å‡∏£‡∏≤‡∏ü R¬≤ Score
    ax[1].bar(results.keys(), [res["R¬≤ Score"]
                               for res in results.values()], color=['red', 'blue', 'green'])
    ax[1].set_title("R¬≤ Score")
    ax[1].set_ylabel("Score")

    st.pyplot(fig)

elif selected == "üìñ Machine Learning Model Development":
    st.title("üìñ ‡∏Å‡∏≤‡∏£‡∏û‡∏±‡∏í‡∏ô‡∏≤‡πÇ‡∏°‡πÄ‡∏î‡∏• Machine Learning ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏û‡∏¢‡∏≤‡∏Å‡∏£‡∏ì‡πå‡∏£‡∏≤‡∏Ñ‡∏≤ BNB")
    st.header("1. ‡∏Å‡∏≤‡∏£‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• (Data Preparation)")
    st.write("""
‡∏Å‡∏≤‡∏£‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏õ‡πá‡∏ô‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏ó‡∏µ‡πà‡∏ä‡πà‡∏ß‡∏¢‡πÉ‡∏´‡πâ‡πÇ‡∏°‡πÄ‡∏î‡∏• Machine Learning ‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ‡πÅ‡∏•‡∏∞‡∏û‡∏¢‡∏≤‡∏Å‡∏£‡∏ì‡πå‡πÑ‡∏î‡πâ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÅ‡∏°‡πà‡∏ô‡∏¢‡∏≥ ‡πÇ‡∏î‡∏¢‡πÉ‡∏ô‡∏Å‡∏£‡∏ì‡∏µ‡∏ô‡∏µ‡πâ ‡πÄ‡∏£‡∏≤‡πÉ‡∏ä‡πâ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏£‡∏≤‡∏Ñ‡∏≤‡∏Ç‡∏≠‡∏á BNB ‡∏à‡∏≤‡∏Å Binance API ‡πÅ‡∏•‡∏∞‡∏ó‡∏≥‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏±‡∏ö‡πÅ‡∏ï‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏´‡πâ‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏°‡∏Å‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô

### ‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏´‡∏•‡∏±‡∏Å:
1. **‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å Binance API:** ‡πÉ‡∏ä‡πâ‡∏Ñ‡∏≥‡∏Ç‡∏≠ HTTP ‡∏ú‡πà‡∏≤‡∏ô `requests` ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏£‡∏±‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏£‡∏≤‡∏Ñ‡∏≤‡∏Ç‡∏≠‡∏á BNB/USDT
""")
    st.code("""
def fetch_binance_data(symbol="BNBUSDT", interval="1d", limit=1000):
    url = f"https://api.binance.com/api/v3/klines?symbol={symbol}&interval={interval}&limit={limit}"
    response = requests.get(url)
    data = response.json()

    df = pd.DataFrame(data, columns=["timestamp", "open", "high", "low", "close", "volume", "close_time",
                      "quote_asset_volume", "trades", "taker_buy_base_asset_volume", "taker_buy_quote_asset_volume", "ignore"])
    df = df[["timestamp", "open", "high", "low", "close", "volume", "trades"]]
    df["timestamp"] = pd.to_datetime(df["timestamp"], unit="ms")
    df[["open", "high", "low", "close", "volume", "trades"]] = df[[
        "open", "high", "low", "close", "volume", "trades"]].astype(float)
    return df
""", language="python")
    st.write("""
2. **‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•:** ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ñ‡πà‡∏≤‡∏ó‡∏µ‡πà‡∏Ç‡∏≤‡∏î‡∏´‡∏≤‡∏¢ (`NaN`), ‡∏Ñ‡πà‡∏≤‡∏ú‡∏¥‡∏î‡∏õ‡∏Å‡∏ï‡∏¥ (Outliers) ‡πÅ‡∏•‡∏∞‡∏à‡∏±‡∏î‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö Timestamp
3. **‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ü‡∏µ‡πÄ‡∏à‡∏≠‡∏£‡πå‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏ï‡∏¥‡∏° (Feature Engineering):** ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏ï‡∏±‡∏ß‡∏ä‡∏µ‡πâ‡∏ß‡∏±‡∏î‡∏ó‡∏≤‡∏á‡πÄ‡∏ó‡∏Ñ‡∏ô‡∏¥‡∏Ñ ‡πÄ‡∏ä‡πà‡∏ô
   - **Moving Average (MA):** ‡∏Ñ‡πà‡∏≤‡πÄ‡∏â‡∏•‡∏µ‡πà‡∏¢‡∏Ç‡∏≠‡∏á‡∏£‡∏≤‡∏Ñ‡∏≤‡∏¢‡πâ‡∏≠‡∏ô‡∏´‡∏•‡∏±‡∏á‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏´‡∏≤‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏Ñ‡∏•‡∏∑‡πà‡∏≠‡∏ô‡πÑ‡∏´‡∏ß‡∏Ç‡∏≠‡∏á‡πÅ‡∏ô‡∏ß‡πÇ‡∏ô‡πâ‡∏°‡∏ï‡∏•‡∏≤‡∏î
   - **Exponential Moving Average (EMA):** ‡∏Ñ‡∏•‡πâ‡∏≤‡∏¢ MA ‡πÅ‡∏ï‡πà‡πÉ‡∏´‡πâ‡∏ô‡πâ‡∏≥‡∏´‡∏ô‡∏±‡∏Å‡∏°‡∏≤‡∏Å‡∏Ç‡∏∂‡πâ‡∏ô‡∏Å‡∏±‡∏ö‡∏Ñ‡πà‡∏≤‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î
   - **Relative Strength Index (RSI):** ‡∏ß‡∏±‡∏î‡∏ß‡πà‡∏≤‡∏´‡∏∏‡πâ‡∏ô‡∏ñ‡∏π‡∏Å‡∏ã‡∏∑‡πâ‡∏≠‡∏°‡∏≤‡∏Å‡πÄ‡∏Å‡∏¥‡∏ô‡πÑ‡∏õ (Overbought) ‡∏´‡∏£‡∏∑‡∏≠‡∏Ç‡∏≤‡∏¢‡∏°‡∏≤‡∏Å‡πÄ‡∏Å‡∏¥‡∏ô‡πÑ‡∏õ (Oversold)
""")
    st.code("""
def add_indicators(df):
    df["MA_7"] = df["close"].rolling(window=7).mean()
    df["MA_14"] = df["close"].rolling(window=14).mean()
    df["EMA_7"] = df["close"].ewm(span=7, adjust=False).mean()
    df["EMA_14"] = df["close"].ewm(span=14, adjust=False).mean()
    df["RSI_14"] = 100 - (100 / (1 + (df["close"].diff(1).clip(lower=0).rolling(
        14).mean() / df["close"].diff(1).clip(upper=0).abs().rolling(14).mean())))
    df["Daily_Return"] = df["close"].pct_change() * 100
    df.dropna(inplace=True)
    return df
""", language="python")
    st.write("""
4. **‡∏Å‡∏≤‡∏£‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡∏Ñ‡πà‡∏≤‡∏ó‡∏µ‡πà‡∏´‡∏≤‡∏¢‡πÑ‡∏õ:** ‡πÉ‡∏ä‡πâ‡πÄ‡∏ó‡∏Ñ‡∏ô‡∏¥‡∏Ñ‡∏Å‡∏≤‡∏£‡πÄ‡∏ï‡∏¥‡∏°‡∏Ñ‡πà‡∏≤ (`imputation`) ‡∏´‡∏£‡∏∑‡∏≠‡∏Å‡∏≤‡∏£‡∏•‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô
5. **‡∏Å‡∏≤‡∏£ Normalize ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•:** ‡πÉ‡∏ä‡πâ StandardScaler ‡∏´‡∏£‡∏∑‡∏≠ MinMaxScaler ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ó‡∏≥‡πÉ‡∏´‡πâ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô‡∏ä‡πà‡∏ß‡∏á‡∏ó‡∏µ‡πà‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏°‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÇ‡∏°‡πÄ‡∏î‡∏•
""")
    st.code("""
def train_and_save_model(df, symbol):
    features = ["MA_7", "MA_14", "EMA_7", "EMA_14", "RSI_14", "Daily_Return"]
    target = "close"
    X = df[features]
    y = df[target]

    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42)
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)

    models = {
        "Linear Regression": LinearRegression(),
        "Random Forest": RandomForestRegressor(n_estimators=100, random_state=42),
        "XGBoost": XGBRegressor(n_estimators=100, learning_rate=0.1, random_state=42)
    }

    results = {}  # ‡πÄ‡∏Å‡πá‡∏ö‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏Ç‡∏≠‡∏á‡πÅ‡∏ï‡πà‡∏•‡∏∞‡πÇ‡∏°‡πÄ‡∏î‡∏•
    for name, model in models.items():
        model.fit(X_train_scaled, y_train)
        y_pred = model.predict(X_test_scaled)
        mae = mean_absolute_error(y_test, y_pred)
        r2 = r2_score(y_test, y_pred)
        results[name] = {"MAE": mae, "R¬≤ Score": r2}

    best_model_name = max(results, key=lambda x: results[x]["R¬≤ Score"])
    best_model = models[best_model_name]
    model_filename = f"{symbol.lower()}_best_model.pkl"

    joblib.dump(best_model, model_filename)  # ‚úÖ ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÇ‡∏°‡πÄ‡∏î‡∏•
    joblib.dump(scaler, f"{symbol.lower()}_scaler.pkl")  # ‚úÖ ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å scaler

    return model_filename, results
""", language="python")

    st.header("2. ‡∏ó‡∏§‡∏©‡∏é‡∏µ‡∏Ç‡∏≠‡∏á‡∏≠‡∏±‡∏•‡∏Å‡∏≠‡∏£‡∏¥‡∏ó‡∏∂‡∏° (Machine Learning Algorithms)")

    st.subheader("üìà Linear Regression")
    st.write("""
**Linear Regression** ‡πÉ‡∏ä‡πâ‡∏™‡∏°‡∏Å‡∏≤‡∏£‡πÄ‡∏ä‡∏¥‡∏á‡πÄ‡∏™‡πâ‡∏ô‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏û‡∏¢‡∏≤‡∏Å‡∏£‡∏ì‡πå‡∏£‡∏≤‡∏Ñ‡∏≤‡∏õ‡∏¥‡∏î‡∏Ç‡∏≠‡∏á BNB ‡πÇ‡∏î‡∏¢‡∏≠‡∏≤‡∏®‡∏±‡∏¢‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏±‡∏°‡∏û‡∏±‡∏ô‡∏ò‡πå‡πÄ‡∏ä‡∏¥‡∏á‡πÄ‡∏™‡πâ‡∏ô‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á‡∏ü‡∏µ‡πÄ‡∏à‡∏≠‡∏£‡πå (‡∏ï‡∏±‡∏ß‡∏ä‡∏µ‡πâ‡∏ß‡∏±‡∏î‡∏ó‡∏≤‡∏á‡πÄ‡∏ó‡∏Ñ‡∏ô‡∏¥‡∏Ñ) ‡πÅ‡∏•‡∏∞‡∏£‡∏≤‡∏Ñ‡∏≤‡∏õ‡∏¥‡∏î

### ‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡∏¢‡∏∏‡∏Å‡∏ï‡πå‡πÉ‡∏ä‡πâ‡∏Å‡∏±‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• BNB:
- ‡πÄ‡∏£‡∏≤‡πÉ‡∏ä‡πâ‡∏Ñ‡πà‡∏≤ **Moving Average (MA), Exponential Moving Average (EMA), ‡πÅ‡∏•‡∏∞ RSI** ‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡∏±‡∏ß‡πÅ‡∏õ‡∏£‡∏≠‡∏¥‡∏™‡∏£‡∏∞ (Independent Variables)
- ‡πÉ‡∏ä‡πâ **‡∏£‡∏≤‡∏Ñ‡∏≤‡∏õ‡∏¥‡∏î‡∏Ç‡∏≠‡∏á BNB (Close Price)** ‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡∏±‡∏ß‡πÅ‡∏õ‡∏£‡∏ï‡∏≤‡∏° (Dependent Variable)
- ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏à‡∏∞‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ‡πÅ‡∏ô‡∏ß‡πÇ‡∏ô‡πâ‡∏°‡∏£‡∏≤‡∏Ñ‡∏≤‡∏à‡∏≤‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏¢‡πâ‡∏≠‡∏ô‡∏´‡∏•‡∏±‡∏á ‡πÅ‡∏•‡∏∞‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏£‡∏≤‡∏Ñ‡∏≤‡∏õ‡∏¥‡∏î‡∏Ç‡∏≠‡∏á‡∏ß‡∏±‡∏ô‡∏ñ‡∏±‡∏î‡πÑ‡∏õ

‡∏Ç‡πâ‡∏≠‡∏à‡∏≥‡∏Å‡∏±‡∏î: Linear Regression ‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏Å‡∏±‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏°‡∏µ‡πÅ‡∏ô‡∏ß‡πÇ‡∏ô‡πâ‡∏°‡πÄ‡∏ä‡∏¥‡∏á‡πÄ‡∏™‡πâ‡∏ô‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô ‡πÅ‡∏ï‡πà‡∏ï‡∏•‡∏≤‡∏î‡∏Ñ‡∏£‡∏¥‡∏õ‡πÇ‡∏ï‡∏°‡∏±‡∏Å‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ã‡∏±‡∏ö‡∏ã‡πâ‡∏≠‡∏ô‡πÅ‡∏•‡∏∞‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ú‡∏±‡∏ô‡∏ú‡∏ß‡∏ô‡∏™‡∏π‡∏á ‡∏≠‡∏≤‡∏à‡∏ó‡∏≥‡πÉ‡∏´‡πâ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏û‡∏¢‡∏≤‡∏Å‡∏£‡∏ì‡πå‡πÑ‡∏î‡πâ‡πÑ‡∏°‡πà‡πÅ‡∏°‡πà‡∏ô‡∏¢‡∏≥‡πÄ‡∏°‡∏∑‡πà‡∏≠‡πÅ‡∏ô‡∏ß‡πÇ‡∏ô‡πâ‡∏°‡πÑ‡∏°‡πà‡πÄ‡∏õ‡πá‡∏ô‡πÄ‡∏™‡πâ‡∏ô‡∏ï‡∏£‡∏á
""")

    st.subheader("üå≤ Random Forest")
    st.write("""
**Random Forest** ‡πÄ‡∏õ‡πá‡∏ô‡∏≠‡∏±‡∏•‡∏Å‡∏≠‡∏£‡∏¥‡∏ó‡∏∂‡∏°‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ‡∏Å‡∏≤‡∏£‡∏£‡∏ß‡∏°‡∏ú‡∏•‡∏Ç‡∏≠‡∏á‡∏´‡∏•‡∏≤‡∏¢ ‡πÜ Decision Tree ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏•‡∏î‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÅ‡∏•‡∏∞‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏°‡πà‡∏ô‡∏¢‡∏≥

### ‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡∏¢‡∏∏‡∏Å‡∏ï‡πå‡πÉ‡∏ä‡πâ‡∏Å‡∏±‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• BNB:
- ‡πÉ‡∏ä‡πâ‡∏ï‡∏±‡∏ß‡∏ä‡∏µ‡πâ‡∏ß‡∏±‡∏î‡∏ó‡∏≤‡∏á‡πÄ‡∏ó‡∏Ñ‡∏ô‡∏¥‡∏Ñ ‡πÄ‡∏ä‡πà‡∏ô **MA, EMA, RSI ‡πÅ‡∏•‡∏∞ Daily Return** ‡πÄ‡∏õ‡πá‡∏ô‡∏ü‡∏µ‡πÄ‡∏à‡∏≠‡∏£‡πå
- ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏à‡∏∞‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏´‡∏•‡∏≤‡∏¢ Decision Tree ‡πÅ‡∏•‡∏∞‡∏£‡∏ß‡∏°‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏•‡∏î Overfitting
- ‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏à‡∏±‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏±‡∏°‡∏û‡∏±‡∏ô‡∏ò‡πå‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡πÄ‡∏õ‡πá‡∏ô‡πÄ‡∏ä‡∏¥‡∏á‡πÄ‡∏™‡πâ‡∏ô‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á‡∏ü‡∏µ‡πÄ‡∏à‡∏≠‡∏£‡πå‡πÅ‡∏•‡∏∞‡∏£‡∏≤‡∏Ñ‡∏≤‡∏õ‡∏¥‡∏î‡πÑ‡∏î‡πâ

‡∏Ç‡πâ‡∏≠‡∏î‡∏µ‡∏Ç‡∏≠‡∏á Random Forest ‡∏Ñ‡∏∑‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡∏Å‡∏±‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏°‡∏µ Noise ‡πÅ‡∏•‡∏∞‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ã‡∏±‡∏ö‡∏ã‡πâ‡∏≠‡∏ô‡πÑ‡∏î‡πâ‡∏î‡∏µ ‡∏ó‡∏≥‡πÉ‡∏´‡πâ‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏Å‡∏±‡∏ö‡∏ï‡∏•‡∏≤‡∏î‡∏Ñ‡∏£‡∏¥‡∏õ‡πÇ‡∏ï‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ú‡∏±‡∏ô‡∏ú‡∏ß‡∏ô‡∏™‡∏π‡∏á
""")

    st.subheader("‚ö° XGBoost")
    st.write("""
**XGBoost (Extreme Gradient Boosting)** ‡πÄ‡∏õ‡πá‡∏ô‡∏≠‡∏±‡∏•‡∏Å‡∏≠‡∏£‡∏¥‡∏ó‡∏∂‡∏°‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏õ‡∏£‡∏∞‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡∏†‡∏≤‡∏û‡∏™‡∏π‡∏á‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏û‡∏¢‡∏≤‡∏Å‡∏£‡∏ì‡πå‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ã‡∏±‡∏ö‡∏ã‡πâ‡∏≠‡∏ô‡πÅ‡∏•‡∏∞‡∏°‡∏µ‡πÅ‡∏ô‡∏ß‡πÇ‡∏ô‡πâ‡∏°‡πÑ‡∏°‡πà‡πÄ‡∏õ‡πá‡∏ô‡πÄ‡∏™‡πâ‡∏ô‡∏ï‡∏£‡∏á

### ‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡∏¢‡∏∏‡∏Å‡∏ï‡πå‡πÉ‡∏ä‡πâ‡∏Å‡∏±‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• BNB:
- ‡πÉ‡∏ä‡πâ‡∏ï‡∏±‡∏ß‡∏ä‡∏µ‡πâ‡∏ß‡∏±‡∏î‡∏ó‡∏≤‡∏á‡πÄ‡∏ó‡∏Ñ‡∏ô‡∏¥‡∏Ñ ‡πÄ‡∏ä‡πà‡∏ô MA, EMA, RSI, ‡πÅ‡∏•‡∏∞ Daily Return ‡πÄ‡∏õ‡πá‡∏ô‡∏ü‡∏µ‡πÄ‡∏à‡∏≠‡∏£‡πå
- ‡πÉ‡∏ä‡πâ‡πÄ‡∏ó‡∏Ñ‡∏ô‡∏¥‡∏Ñ Gradient Boosting ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ñ‡πà‡∏≠‡∏¢ ‡πÜ ‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏Ç‡∏≠‡∏á‡∏ï‡πâ‡∏ô‡πÑ‡∏°‡πâ‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏ï‡πâ‡∏ô ‡∏ó‡∏≥‡πÉ‡∏´‡πâ‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏à‡∏±‡∏ö‡πÅ‡∏ô‡∏ß‡πÇ‡∏ô‡πâ‡∏°‡∏ó‡∏µ‡πà‡∏ã‡∏±‡∏ö‡∏ã‡πâ‡∏≠‡∏ô‡∏Ç‡∏≠‡∏á‡∏ï‡∏•‡∏≤‡∏î‡∏Ñ‡∏£‡∏¥‡∏õ‡πÇ‡∏ï‡πÑ‡∏î‡πâ
- ‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡∏Å‡∏±‡∏ö‡∏Ñ‡πà‡∏≤‡∏ó‡∏µ‡πà‡∏Ç‡∏≤‡∏î‡∏´‡∏≤‡∏¢‡πÑ‡∏õ ‡πÅ‡∏•‡∏∞‡πÉ‡∏´‡πâ‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏ó‡∏µ‡πà‡πÅ‡∏°‡πà‡∏ô‡∏¢‡∏≥‡∏Å‡∏ß‡πà‡∏≤‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏≠‡∏∑‡πà‡∏ô ‡πÜ

‡∏Ç‡πâ‡∏≠‡∏î‡∏µ: XGBoost ‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏±‡∏°‡∏û‡∏±‡∏ô‡∏ò‡πå‡∏ó‡∏µ‡πà‡∏ã‡∏±‡∏ö‡∏ã‡πâ‡∏≠‡∏ô‡∏Ç‡∏≠‡∏á‡∏ï‡∏•‡∏≤‡∏î‡πÅ‡∏•‡∏∞‡∏•‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡∏Ç‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏û‡∏¢‡∏≤‡∏Å‡∏£‡∏ì‡πå‡πÑ‡∏î‡πâ‡∏î‡∏µ ‡∏à‡∏∂‡∏á‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏ó‡∏µ‡πà‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏°‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏£‡∏≤‡∏Ñ‡∏≤‡∏Ñ‡∏£‡∏¥‡∏õ‡πÇ‡∏ï
""")

    st.header("3. ‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏Å‡∏≤‡∏£‡∏û‡∏±‡∏í‡∏ô‡∏≤‡πÇ‡∏°‡πÄ‡∏î‡∏• (Model Development Steps)")
    st.subheader("üîπ ‡πÅ‡∏ö‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏õ‡πá‡∏ô Train ‡πÅ‡∏•‡∏∞ Test")
    st.write("""
‡πÄ‡∏£‡∏≤‡∏ó‡∏≥‡∏Å‡∏≤‡∏£‡πÅ‡∏ö‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏≠‡∏≠‡∏Å‡πÄ‡∏õ‡πá‡∏ô 80% ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏ù‡∏∂‡∏Å‡πÇ‡∏°‡πÄ‡∏î‡∏• (Training Set) ‡πÅ‡∏•‡∏∞ 20% ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏ó‡∏î‡∏™‡∏≠‡∏ö (Test Set) ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏Ç‡∏≠‡∏á‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏û‡∏¢‡∏≤‡∏Å‡∏£‡∏ì‡πå‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏´‡∏°‡πà‡πÑ‡∏î‡πâ
""")
    st.code("""
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
""", language="python")

    st.subheader("üîπ ‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏±‡∏ö‡πÅ‡∏ï‡πà‡∏á Hyperparameter")

    st.write("""
Hyperparameter ‡∏Ñ‡∏∑‡∏≠‡∏Ñ‡πà‡∏≤‡∏ï‡πà‡∏≤‡∏á ‡πÜ ‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏≤‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡πÄ‡∏≠‡∏á‡πÑ‡∏î‡πâ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏õ‡∏£‡∏∞‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡∏†‡∏≤‡∏û‡∏Ç‡∏≠‡∏á‡πÇ‡∏°‡πÄ‡∏î‡∏• ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÄ‡∏ä‡πà‡∏ô:
- **Linear Regression:**
  - `fit_intercept`: ‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏ß‡πà‡∏≤‡∏à‡∏∞‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏Ñ‡πà‡∏≤‡∏ï‡∏±‡∏î‡πÅ‡∏Å‡∏ô (Intercept) ‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
  - `normalize`: ‡∏£‡∏∞‡∏ö‡∏∏‡∏ß‡πà‡∏≤‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Ñ‡∏ß‡∏£‡∏ñ‡∏π‡∏Å‡∏õ‡∏£‡∏±‡∏ö‡∏Ç‡∏ô‡∏≤‡∏î‡∏Å‡πà‡∏≠‡∏ô‡∏Å‡∏≤‡∏£‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
""")
    st.code("""
from sklearn.linear_model import LinearRegression
lr_model = LinearRegression(fit_intercept=True, normalize=True)
lr_model.fit(X_train, y_train)
""", language="python")
    st.write("""
- **Random Forest:**
  - `n_estimators`: ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏ï‡πâ‡∏ô‡πÑ‡∏°‡πâ‡πÉ‡∏ô‡∏õ‡πà‡∏≤
  - `max_depth`: ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏•‡∏∂‡∏Å‡∏Ç‡∏≠‡∏á‡∏ï‡πâ‡∏ô‡πÑ‡∏°‡πâ‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏ï‡πâ‡∏ô
  - `min_samples_split`: ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡πà‡∏≥‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏µ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÅ‡∏ö‡πà‡∏á‡πÇ‡∏´‡∏ô‡∏î
""")
    st.code("""
from sklearn.ensemble import RandomForestRegressor
rf_model = RandomForestRegressor(n_estimators=100, max_depth=10, min_samples_split=5, random_state=42)
rf_model.fit(X_train, y_train)
""", language="python")
    st.write("""
- **XGBoost:**
  - `learning_rate`: ‡∏≠‡∏±‡∏ï‡∏£‡∏≤‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ
  - `n_estimators`: ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏£‡∏≠‡∏ö‡∏Ç‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ
  - `max_depth`: ‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ã‡∏±‡∏ö‡∏ã‡πâ‡∏≠‡∏ô‡∏Ç‡∏≠‡∏á‡πÇ‡∏°‡πÄ‡∏î‡∏•
""")

    st.code("""
from xgboost import XGBRegressor
xgb_model = XGBRegressor(learning_rate=0.1, n_estimators=200, max_depth=6, random_state=42)
xgb_model.fit(X_train, y_train)
""", language="python")

    st.header("4. ‡∏Å‡∏≤‡∏£‡∏ô‡∏≥‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÑ‡∏õ‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô (Deployment & Prediction)")
    st.write("""
‡πÄ‡∏°‡∏∑‡πà‡∏≠‡∏ô‡∏≥‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÑ‡∏õ‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô ‡πÄ‡∏£‡∏≤‡∏à‡∏∞:
- ‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ó‡∏µ‡πà‡∏î‡∏µ‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î‡∏à‡∏≤‡∏Å‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏µ‡πà‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÑ‡∏ß‡πâ (`joblib.load`)
- ‡∏£‡∏±‡∏ö‡∏Ñ‡πà‡∏≤‡∏ï‡∏±‡∏ß‡πÅ‡∏õ‡∏£‡∏≠‡∏¥‡∏™‡∏£‡∏∞‡∏à‡∏≤‡∏Å‡∏ú‡∏π‡πâ‡πÉ‡∏ä‡πâ‡∏ú‡πà‡∏≤‡∏ô Streamlit
- ‡∏ó‡∏≥‡∏Å‡∏≤‡∏£‡∏û‡∏¢‡∏≤‡∏Å‡∏£‡∏ì‡πå‡∏£‡∏≤‡∏Ñ‡∏≤ BNB ‡πÇ‡∏î‡∏¢‡πÉ‡∏ä‡πâ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ó‡∏µ‡πà‡πÇ‡∏´‡∏•‡∏î‡∏°‡∏≤
- ‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡πÄ‡∏õ‡πá‡∏ô‡∏Ñ‡πà‡∏≤‡∏ó‡∏µ‡πà‡∏û‡∏¢‡∏≤‡∏Å‡∏£‡∏ì‡πå‡∏≠‡∏≠‡∏Å‡∏°‡∏≤ ‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏Å‡∏£‡∏≤‡∏ü‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö‡∏Å‡∏±‡∏ö‡∏£‡∏≤‡∏Ñ‡∏≤‡∏à‡∏£‡∏¥‡∏á
""")


elif selected == "üìàUS Stock Price Prediction (NN model)":
    st.title("üìà‡πäUS Stock Price Prediction")
    # ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÇ‡∏°‡πÄ‡∏î‡∏•
    model_option = st.sidebar.radio("‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ‡∏û‡∏¢‡∏≤‡∏Å‡∏£‡∏ì‡πå‡∏£‡∏≤‡∏Ñ‡∏≤:", ["CNN", "LSTM"])
    stock_symbol = st.sidebar.text_input("‡∏õ‡πâ‡∏≠‡∏ô‡∏™‡∏±‡∏ç‡∏•‡∏±‡∏Å‡∏©‡∏ì‡πå‡∏´‡∏∏‡πâ‡∏ô (‡πÄ‡∏ä‡πà‡∏ô AAPL, TSLA, MSFT):", value="AAPL").upper()
    
    if st.sidebar.button(f"üîç ‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢ {stock_symbol}"):
        with st.spinner(f"‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Ç‡∏≠‡∏á {stock_symbol}..."):
            from datetime import datetime
            current_date = datetime.today().strftime('%Y-%m-%d')
            df = yf.download(stock_symbol, start='2010-01-01', end=current_date)
            df = df[['Open', 'High', 'Low', 'Close', 'Volume']]
            df.dropna(inplace=True)
            
            scaler = MinMaxScaler(feature_range=(0,1))
            scaled_data = scaler.fit_transform(df)
            
            def create_sequences(data, time_steps=30):
                X, y = [], []
                for i in range(len(data) - time_steps):
                    X.append(data[i:i + time_steps])
                    y.append(data[i + time_steps, 3])  
                return np.array(X), np.array(y)
            
            X, y = create_sequences(scaled_data)
            X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, shuffle=False)
            
            early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)
            
            if model_option == "CNN":
                model = Sequential([
                    Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=(X_train.shape[1], X_train.shape[2])),
                    BatchNormalization(),
                    MaxPooling1D(pool_size=2),
                    Flatten(),
                    Dense(50, activation='relu'),
                    Dense(1)
                ])
            else:
                model = Sequential([
                    LSTM(64, return_sequences=True, activation='tanh', recurrent_activation='sigmoid', unroll=True, input_shape=(X_train.shape[1], X_train.shape[2])),
                    Dropout(0.2),
                    LSTM(32, return_sequences=False, activation='tanh', recurrent_activation='sigmoid', unroll=True),
                    Dense(25, activation='relu'),
                    Dense(1)
                ])
            
            model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0005), loss='mean_squared_error')
            history = model.fit(X_train, y_train, epochs=50, batch_size=16, validation_data=(X_test, y_test), callbacks=[early_stopping])
            
            predicted_prices = model.predict(X_test)
            predicted_prices = scaler.inverse_transform(np.concatenate((np.zeros((predicted_prices.shape[0], 3)), predicted_prices, np.zeros((predicted_prices.shape[0], 1))), axis=1))[:, 3]
            
            st.success(f"üí∞ ‡∏£‡∏≤‡∏Ñ‡∏≤‡∏ó‡∏µ‡πà‡∏Ñ‡∏≤‡∏î‡∏Å‡∏≤‡∏£‡∏ì‡πå‡∏Ç‡∏≠‡∏á {stock_symbol}: ${predicted_prices[-1]:.2f}")
            
            st.subheader(f"üìä Actual vs Predicted Stock Prices {stock_symbol}")
            fig, ax = plt.subplots(figsize=(10, 5))
            ax.plot(df.index[-90:], df['Close'].values[-90:], label="Actual Price", color="blue")
            ax.plot(df.index[-90:], predicted_prices[-90:], label="Predicted Price", color="red", linestyle="dashed")
            ax.set_xlabel("Date")
            latest_date = df.index[-1].strftime('%Y-%m-%d')
            ax.annotate(f'Latest: {latest_date}', xy=(df.index[-1], df['Close'].values[-1]), xytext=(-50,10), textcoords='offset points', arrowprops=dict(arrowstyle='->', color='black'), fontsize=10, color='black')
            ax.xaxis.set_major_locator(plt.MaxNLocator(6))
            plt.xticks(rotation=45)
            ax.set_ylabel("Price (USD)")
            ax.legend()
            st.pyplot(fig)
            
            st.subheader("üìâ Training & Validation Loss")
            fig, ax = plt.subplots(figsize=(8, 5))
            ax.plot(history.history['loss'], label='Training Loss', color='blue')
            ax.plot(history.history['val_loss'], label='Validation Loss', color='red', linestyle='dashed')
            ax.set_xlabel("Epochs")
            ax.set_ylabel("Loss")
            ax.legend()
            st.pyplot(fig)

elif selected == "üìñ Neural Network Model Development":
    st.title("üìñ Neural Network Model Development")
    st.write("""
    ‡πÉ‡∏ô‡∏´‡∏ô‡πâ‡∏≤‡∏ô‡∏µ‡πâ‡πÄ‡∏£‡∏≤‡∏à‡∏∞‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢ **‡πÅ‡∏ô‡∏ß‡∏Ñ‡∏¥‡∏î, ‡∏ó‡∏§‡∏©‡∏é‡∏µ, ‡πÅ‡∏•‡∏∞‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏Å‡∏≤‡∏£‡∏û‡∏±‡∏í‡∏ô‡∏≤‡πÇ‡∏°‡πÄ‡∏î‡∏• Neural Network**  
    ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏û‡∏¢‡∏≤‡∏Å‡∏£‡∏ì‡πå‡∏£‡∏≤‡∏Ñ‡∏≤‡∏´‡∏∏‡πâ‡∏ô AAPL ‡πÇ‡∏î‡∏¢‡πÉ‡∏ä‡πâ **LSTM ‡πÅ‡∏•‡∏∞ CNN**  
    """)

    # üîπ **1. ‡∏Å‡∏≤‡∏£‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• (Data Preparation)**
    st.header("üîπ 1. ‡∏Å‡∏≤‡∏£‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• (Data Preparation)")
    st.write("""
    - ‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏´‡∏∏‡πâ‡∏ô AAPL ‡∏à‡∏≤‡∏Å **Yahoo Finance API**  
    - ‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• ‡πÅ‡∏•‡∏∞‡∏õ‡∏£‡∏±‡∏ö‡∏Ç‡∏ô‡∏≤‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• (Normalization)  
    """)

    st.code("""
    import yfinance as yf
    import pandas as pd
    from sklearn.preprocessing import MinMaxScaler

    # ‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏´‡∏∏‡πâ‡∏ô AAPL
    df = yf.download('AAPL', start='2010-01-01', end='2025-03-16')

    # ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏ó‡∏µ‡πà‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç
    df = df[['Open', 'High', 'Low', 'Close', 'Volume']].interpolate(method='linear')

    # ‡∏õ‡∏£‡∏±‡∏ö‡∏Ç‡∏ô‡∏≤‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
    scaler = MinMaxScaler(feature_range=(0,1))
    scaled_data = scaler.fit_transform(df)
    """, language="python")

    # üîπ **2. ‡∏ó‡∏§‡∏©‡∏é‡∏µ‡∏Ç‡∏≠‡∏á‡∏≠‡∏±‡∏•‡∏Å‡∏≠‡∏£‡∏¥‡∏ó‡∏∂‡∏° (LSTM & CNN)**
    st.header("üîπ 2. ‡∏ó‡∏§‡∏©‡∏é‡∏µ‡∏Ç‡∏≠‡∏á‡∏≠‡∏±‡∏•‡∏Å‡∏≠‡∏£‡∏¥‡∏ó‡∏∂‡∏° (LSTM & CNN)")

    ## üî∏ LSTM (Long Short-Term Memory)
    st.subheader("üî∏ LSTM (Long Short-Term Memory)")
    st.write("""
    **LSTM** ‡πÄ‡∏õ‡πá‡∏ô Neural Network ‡∏ó‡∏µ‡πà‡∏≠‡∏≠‡∏Å‡πÅ‡∏ö‡∏ö‡∏°‡∏≤‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÅ‡∏ö‡∏ö‡∏•‡∏≥‡∏î‡∏±‡∏ö (Sequential Data)  
    ‡∏ã‡∏∂‡πà‡∏á‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Time Series ‡πÄ‡∏ä‡πà‡∏ô ‡∏£‡∏≤‡∏Ñ‡∏≤‡∏´‡∏∏‡πâ‡∏ô‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÅ‡∏õ‡∏•‡∏á‡∏ï‡∏≤‡∏°‡πÄ‡∏ß‡∏•‡∏≤  

    **‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡∏¢‡∏∏‡∏Å‡∏ï‡πå‡πÉ‡∏ä‡πâ‡∏Å‡∏±‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• AAPL:**  
    - ‡πÉ‡∏ä‡πâ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏¢‡πâ‡∏≠‡∏ô‡∏´‡∏•‡∏±‡∏á (‡πÄ‡∏ä‡πà‡∏ô 30 ‡∏ß‡∏±‡∏ô) ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏£‡∏≤‡∏Ñ‡∏≤‡∏´‡∏∏‡πâ‡∏ô‡πÉ‡∏ô‡∏ß‡∏±‡∏ô‡∏ñ‡∏±‡∏î‡πÑ‡∏õ  
    - ‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏à‡∏±‡∏ö **‡πÅ‡∏ô‡∏ß‡πÇ‡∏ô‡πâ‡∏°‡∏£‡∏≤‡∏Ñ‡∏≤‡∏´‡∏∏‡πâ‡∏ô‡πÉ‡∏ô‡∏£‡∏∞‡∏¢‡∏∞‡∏¢‡∏≤‡∏ß** ‡πÑ‡∏î‡πâ  
    """)

    ## üî∏ CNN (Convolutional Neural Network)
    st.subheader("üî∏ CNN (Convolutional Neural Network)")
    st.write("""
    **CNN** ‡πÉ‡∏ä‡πâ Convolutional Layers ‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ **‡∏•‡∏±‡∏Å‡∏©‡∏ì‡∏∞‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏Ç‡∏≠‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•**  
    ‡πÅ‡∏°‡πâ CNN ‡∏°‡∏±‡∏Å‡πÉ‡∏ä‡πâ‡∏Å‡∏±‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û ‡πÅ‡∏ï‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÉ‡∏ä‡πâ‡∏Å‡∏±‡∏ö **Time Series** ‡πÑ‡∏î‡πâ‡πÇ‡∏î‡∏¢‡πÉ‡∏ä‡πâ **Conv1D**  

    **‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡∏¢‡∏∏‡∏Å‡∏ï‡πå‡πÉ‡∏ä‡πâ‡∏Å‡∏±‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• AAPL:**  
    - ‡πÉ‡∏ä‡πâ CNN ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡∏Ç‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡πÄ‡∏Ñ‡∏•‡∏∑‡πà‡∏≠‡∏ô‡πÑ‡∏´‡∏ß‡∏Ç‡∏≠‡∏á‡∏£‡∏≤‡∏Ñ‡∏≤  
    - ‡∏î‡∏∂‡∏á‡∏•‡∏±‡∏Å‡∏©‡∏ì‡∏∞‡πÄ‡∏î‡πà‡∏ô‡∏Ç‡∏≠‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏´‡∏∏‡πâ‡∏ô ‡πÄ‡∏ä‡πà‡∏ô **‡πÅ‡∏ô‡∏ß‡πÇ‡∏ô‡πâ‡∏°‡∏£‡∏≤‡∏Ñ‡∏≤ ‡πÅ‡∏•‡∏∞‡∏õ‡∏£‡∏¥‡∏°‡∏≤‡∏ì‡∏Å‡∏≤‡∏£‡∏ã‡∏∑‡πâ‡∏≠‡∏Ç‡∏≤‡∏¢**  
    """)
    
    # üîπ **3. ‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏Å‡∏≤‡∏£‡∏û‡∏±‡∏í‡∏ô‡∏≤‡πÇ‡∏°‡πÄ‡∏î‡∏• (Model Development Steps)**
    st.header("üîπ 3. ‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏Å‡∏≤‡∏£‡∏û‡∏±‡∏í‡∏ô‡∏≤‡πÇ‡∏°‡πÄ‡∏î‡∏•")
    st.write("""
    ‡πÉ‡∏ô‡∏™‡πà‡∏ß‡∏ô‡∏ô‡∏µ‡πâ ‡πÄ‡∏£‡∏≤‡∏à‡∏∞‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏û‡∏±‡∏í‡∏ô‡∏≤‡πÇ‡∏°‡πÄ‡∏î‡∏• Neural Network  
    ‡∏ï‡∏±‡πâ‡∏á‡πÅ‡∏ï‡πà‡∏Å‡∏≤‡∏£‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• ‡∏à‡∏ô‡∏ñ‡∏∂‡∏á‡∏Å‡∏≤‡∏£‡πÄ‡∏ó‡∏£‡∏ô ‡πÅ‡∏•‡∏∞‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡∏ú‡∏•‡πÇ‡∏°‡πÄ‡∏î‡∏•  
    """)



    ## üî∏ 3.1 ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ä‡∏∏‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• Time Series
    st.subheader("üîπ 3.1 ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ä‡∏∏‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• Time Series")
    st.write("""
    - ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏´‡∏∏‡πâ‡∏ô‡πÄ‡∏õ‡πá‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÅ‡∏ö‡∏ö **Time Series** (‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÅ‡∏õ‡∏•‡∏á‡∏ï‡∏≤‡∏°‡πÄ‡∏ß‡∏•‡∏≤)  
    - ‡πÇ‡∏°‡πÄ‡∏î‡∏• LSTM ‡πÅ‡∏•‡∏∞ CNN ‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£ **‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏¢‡πâ‡∏≠‡∏ô‡∏´‡∏•‡∏±‡∏á** ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏ä‡πâ‡πÄ‡∏õ‡πá‡∏ô‡∏≠‡∏¥‡∏ô‡∏û‡∏∏‡∏ï  
    - ‡πÄ‡∏£‡∏≤‡∏à‡∏∞‡πÉ‡∏ä‡πâ **30 ‡∏ß‡∏±‡∏ô‡∏¢‡πâ‡∏≠‡∏ô‡∏´‡∏•‡∏±‡∏á** ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ä‡πà‡∏ß‡∏¢‡∏û‡∏¢‡∏≤‡∏Å‡∏£‡∏ì‡πå‡∏£‡∏≤‡∏Ñ‡∏≤‡∏´‡∏∏‡πâ‡∏ô‡πÉ‡∏ô‡∏≠‡∏ô‡∏≤‡∏Ñ‡∏ï  
    """)

    st.code("""
    import numpy as np

    def create_sequences(data, time_steps=30):
        X, y = [], []
        for i in range(len(data) - time_steps):
            X.append(data[i:i + time_steps])
            y.append(data[i + time_steps, 3])  # ‡∏£‡∏≤‡∏Ñ‡∏≤‡∏õ‡∏¥‡∏î
        return np.array(X), np.array(y)

    # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ä‡∏∏‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
    X, y = create_sequences(scaled_data, time_steps=30)
    """, language="python")

    ## üî∏ 3.2 ‡πÅ‡∏ö‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏õ‡πá‡∏ô Train & Test
    st.subheader("üîπ 3.2 ‡πÅ‡∏ö‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏õ‡πá‡∏ô Train & Test")
    st.write("""
    - ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡∏à‡∏∞‡∏ñ‡∏π‡∏Å‡πÅ‡∏ö‡πà‡∏á‡πÄ‡∏õ‡πá‡∏ô **2 ‡∏™‡πà‡∏ß‡∏ô:**  
      1. **Training Set (80%)** ‚Üí ‡πÉ‡∏ä‡πâ‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏ù‡∏∂‡∏Å‡πÇ‡∏°‡πÄ‡∏î‡∏•  
      2. **Testing Set (20%)** ‚Üí ‡πÉ‡∏ä‡πâ‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏Ç‡∏≠‡∏á‡πÇ‡∏°‡πÄ‡∏î‡∏•  
    - ‡πÄ‡∏£‡∏≤‡πÉ‡∏ä‡πâ `shuffle=False` ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏£‡∏±‡∏Å‡∏©‡∏≤‡∏•‡∏≥‡∏î‡∏±‡∏ö‡∏Ç‡∏≠‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• Time Series  
    """)

    st.code("""
    from sklearn.model_selection import train_test_split

    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False, random_state=42)
    """, language="python")

    ## üî∏ 3.3 ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÅ‡∏•‡∏∞‡∏Ñ‡∏≠‡∏°‡πÑ‡∏û‡∏•‡πå‡πÇ‡∏°‡πÄ‡∏î‡∏• LSTM
    st.subheader("üîπ 3.3 ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÅ‡∏•‡∏∞‡∏Ñ‡∏≠‡∏°‡πÑ‡∏û‡∏•‡πå‡πÇ‡∏°‡πÄ‡∏î‡∏• LSTM")
    st.write("""
    - **LSTM (Long Short-Term Memory)** ‡πÄ‡∏õ‡πá‡∏ô‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ó‡∏µ‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏à‡∏î‡∏à‡∏≥‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏ô‡∏£‡∏∞‡∏¢‡∏∞‡∏¢‡∏≤‡∏ß‡πÑ‡∏î‡πâ‡∏î‡∏µ  
    - ‡πÄ‡∏£‡∏≤‡πÉ‡∏ä‡πâ **2 LSTM Layers** ‡πÅ‡∏•‡∏∞‡πÄ‡∏û‡∏¥‡πà‡∏° **Dropout 20%** ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏•‡∏î Overfitting  
    - ‡πÉ‡∏ä‡πâ **Mean Squared Error (MSE)** ‡πÄ‡∏õ‡πá‡∏ô Loss Function  
    """)

    st.code("""
    from tensorflow.keras.models import Sequential
    from tensorflow.keras.layers import LSTM, Dense, Dropout

    lstm_model = Sequential([
        LSTM(64, return_sequences=True, input_shape=(30, 5)),
        Dropout(0.2),
        LSTM(32, return_sequences=False),
        Dense(25, activation='relu'),
        Dense(1)
    ])

    lstm_model.compile(optimizer='adam', loss='mean_squared_error')
    """, language="python")

    ## üî∏ 3.4 ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÅ‡∏•‡∏∞‡∏Ñ‡∏≠‡∏°‡πÑ‡∏û‡∏•‡πå‡πÇ‡∏°‡πÄ‡∏î‡∏• CNN
    st.subheader("üîπ 3.4 ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÅ‡∏•‡∏∞‡∏Ñ‡∏≠‡∏°‡πÑ‡∏û‡∏•‡πå‡πÇ‡∏°‡πÄ‡∏î‡∏• CNN")
    st.write("""
    - **CNN (Convolutional Neural Network)** ‡∏ä‡πà‡∏ß‡∏¢‡∏î‡∏∂‡∏á‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡∏Ç‡∏≠‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏´‡∏∏‡πâ‡∏ô  
    - ‡πÉ‡∏ä‡πâ **Conv1D Layer** ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ï‡∏£‡∏ß‡∏à‡∏à‡∏±‡∏ö‡πÅ‡∏ô‡∏ß‡πÇ‡∏ô‡πâ‡∏°‡∏Ç‡∏≠‡∏á‡∏£‡∏≤‡∏Ñ‡∏≤  
    - ‡πÉ‡∏ä‡πâ **MaxPooling1D** ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏•‡∏î‡∏Ç‡∏ô‡∏≤‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Å‡πà‡∏≠‡∏ô‡∏™‡πà‡∏á‡πÄ‡∏Ç‡πâ‡∏≤ Fully Connected Layer  
    """)

    st.code("""
    from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten

    cnn_model = Sequential([
        Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=(30, 5)),
        MaxPooling1D(pool_size=2),
        Flatten(),
        Dense(50, activation='relu'),
        Dense(1)
    ])

    cnn_model.compile(optimizer='adam', loss='mean_squared_error')
    """, language="python")

    ## üî∏ 3.5 ‡πÄ‡∏ó‡∏£‡∏ô‡πÇ‡∏°‡πÄ‡∏î‡∏•
    st.subheader("üîπ 3.5 ‡πÄ‡∏ó‡∏£‡∏ô‡πÇ‡∏°‡πÄ‡∏î‡∏•")
    st.write("""
    - ‡πÉ‡∏ä‡πâ **Early Stopping** ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏•‡∏î Overfitting ‡πÇ‡∏î‡∏¢‡∏´‡∏¢‡∏∏‡∏î‡πÄ‡∏ó‡∏£‡∏ô‡∏´‡∏≤‡∏Å `val_loss` ‡πÑ‡∏°‡πà‡∏•‡∏î‡∏•‡∏á  
    - ‡πÄ‡∏ó‡∏£‡∏ô‡πÇ‡∏°‡πÄ‡∏î‡∏• **LSTM ‡πÅ‡∏•‡∏∞ CNN** ‡πÇ‡∏î‡∏¢‡πÉ‡∏ä‡πâ **50 epochs**  
    """)

    st.code("""
    from tensorflow.keras.callbacks import EarlyStopping

    early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)

    # ‡πÄ‡∏ó‡∏£‡∏ô‡πÇ‡∏°‡πÄ‡∏î‡∏• LSTM
    lstm_model.fit(X_train, y_train, epochs=50, batch_size=16, validation_data=(X_test, y_test), callbacks=[early_stopping])

    # ‡πÄ‡∏ó‡∏£‡∏ô‡πÇ‡∏°‡πÄ‡∏î‡∏• CNN
    cnn_model.fit(X_train, y_train, epochs=50, batch_size=16, validation_data=(X_test, y_test), callbacks=[early_stopping])
    """, language="python")

    ## üî∏ 3.6 ‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡∏ú‡∏•‡πÇ‡∏°‡πÄ‡∏î‡∏•
    st.subheader("üîπ 3.6 ‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡∏ú‡∏•‡πÇ‡∏°‡πÄ‡∏î‡∏•")
    st.write("""
    - ‡πÉ‡∏ä‡πâ **MAE (Mean Absolute Error)** ‡πÅ‡∏•‡∏∞ **R¬≤ Score** ‡∏ß‡∏±‡∏î‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏°‡πà‡∏ô‡∏¢‡∏≥‡∏Ç‡∏≠‡∏á‡πÇ‡∏°‡πÄ‡∏î‡∏•  
    - ‡∏Ñ‡πà‡∏≤ **MAE ‡∏ï‡πà‡∏≥** ‡πÅ‡∏•‡∏∞ **R¬≤ ‡πÉ‡∏Å‡∏•‡πâ 1** ‡πÅ‡∏™‡∏î‡∏á‡∏ß‡πà‡∏≤‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡πÑ‡∏î‡πâ‡∏î‡∏µ  
    """)

    st.code("""
    from sklearn.metrics import mean_absolute_error, r2_score

    y_pred = lstm_model.predict(X_test)
    mae = mean_absolute_error(y_test, y_pred)
    r2 = r2_score(y_test, y_pred)

    print(f"LSTM - MAE: {mae:.4f}, R¬≤: {r2:.4f}")
    """, language="python")
    
    # üîπ **4. ‡∏Å‡∏≤‡∏£‡∏ô‡∏≥‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÑ‡∏õ‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô (Deployment & Prediction)**
    st.header("üîπ 4. ‡∏Å‡∏≤‡∏£‡∏ô‡∏≥‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÑ‡∏õ‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô (Deployment & Prediction)")
    st.write("""
    ‡πÄ‡∏°‡∏∑‡πà‡∏≠‡∏ô‡∏≥‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÑ‡∏õ‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô ‡πÄ‡∏£‡∏≤‡∏à‡∏∞:
    - **‡∏£‡∏±‡∏ö‡∏Ñ‡πà‡∏≤‡∏ï‡∏±‡∏ß‡πÅ‡∏õ‡∏£‡∏≠‡∏¥‡∏™‡∏£‡∏∞‡∏à‡∏≤‡∏Å‡∏ú‡∏π‡πâ‡πÉ‡∏ä‡πâ‡∏ú‡πà‡∏≤‡∏ô Streamlit**
    - **‡∏ó‡∏≥‡∏Å‡∏≤‡∏£‡∏û‡∏¢‡∏≤‡∏Å‡∏£‡∏ì‡πå‡∏£‡∏≤‡∏Ñ‡∏≤‡∏´‡∏∏‡πâ‡∏ô‡πÇ‡∏î‡∏¢‡πÉ‡∏ä‡πâ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ó‡∏µ‡πà‡∏ú‡∏π‡πâ‡πÉ‡∏ä‡πâ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏°‡∏≤**
    - **‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡πÄ‡∏õ‡πá‡∏ô‡∏Ñ‡πà‡∏≤‡∏ó‡∏µ‡πà‡∏û‡∏¢‡∏≤‡∏Å‡∏£‡∏ì‡πå‡∏≠‡∏≠‡∏Å‡∏°‡∏≤ ‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏Å‡∏£‡∏≤‡∏ü‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö‡∏Å‡∏±‡∏ö‡∏£‡∏≤‡∏Ñ‡∏≤‡∏à‡∏£‡∏¥‡∏á**
    """)
